
Predicting Bike Sharing Patterns

The first Udacity Project from the Deep Learning Nanodegree.
Using only Numpy to implement forward and back propagation to predict Bike-Sharing Patterns.
The Neural network was built from "scratch", using only NumPy to assist. 
The goal of this project is to understand what happens behind the neural network before diving deeper into other tools like PyTorch.

Sherif Sakr Bike Sharing
Cairo, Egypt

Introduction
In this project, I'll get to build a neural network from scratch to carry out a prediction problem on a real dataset! 
By building a neural network from the ground up, I'll have a much better understanding of gradient descent, backpropagation, and other concepts that are important to know before we move to higher-level tools such as PyTorch.
I'll also get to see how to apply these networks to solve real prediction problems!

Background 
- A bicycle-sharing system, public bicycle scheme, or public bike share (PBS) scheme, is a shared transport service in which bicycles are made available for shared use to individuals on a short term basis for a price or free.

- Many bike share systems allow people to borrow a bike from a "dock" and return it at another dock belonging to the same system. Docks are special bike racks that lock the bike, and only release it by computer control. The user enters payment information, and the computer unlocks a bike. The user returns the bike by placing it in the dock, which locks it in place.

- Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return 
back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return 
back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of 
over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, 
environmental and health issues. 

- Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by
these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration
of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into
a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important
events in the city could be detected via monitoring these data.

Deep Learning: Predicting Bike Sharing Data

Functions
- Sigmoid
In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input.
Sigmoid function used as Activation function to introduce non-linearity after hidden layers and when we need outputs to be between 0 and 1.

- Mean Squared Error (MSE)
MSE is the mean of the squares of the differences between the predictions and the labels.
Used as a loss function when output is continuous.

- Forward Propagation
Input layer : Input preprocessed features.
Hidden Layer : Matrix multiply with weight(W) matrices and add Biases(B) followed by activation function (sigmoid) for non-linearity.
Output Layer : Matrix multiply with weight(W) matrices and add Biases(B) followed by activation function (Identity) based on required output.

- Learning Rate
The learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. It controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.

A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck. In general, a good rule of thumb is if your model is not working, decrease the learning rate.

- Other Activation Function
Hyperbolic Tangent
It defined as the ratio between the hyperbolic sine and the cosine functions.
In neural networks, as an alternative to sigmoid function, hyperbolic tangent function could be used as activation function. 
It is similar to sigmoid function, but since the range is between minus one and one, the derivatives are larger.

Rectified Linear Unit (ReLU)
The rectified linear activation function or ReLU for short is a very simple function, It is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.

Feedforward
Feedforward is the process neural networks use to turn the input into an output.

Backpropagation
In a nutshell, the backpropagation will consist of:
Doing a feedforward operation.
Comparing the output of the model with the desired output.
Calculating the error.
Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.
Use this to update the weights, and get a better model.
Continue with this unit we have a model that is good.

SETTING HYPERPARAMETERS
● Iterations :
Increase if both train and validation losses are decreasing.
Stop if validation loss has saturated.
Decrease if validation loss starts increasing.

● Learning_rate :
Increase if train loss is not changing or decreasing fast enough.
Decrease if train loss keeps fluctuating or starts increasing.

● Hidden nodes :
Increasing it makes it harder and longer for the model to train.
Increase when train loss saturates at a high value.
Decrease if train loss does not decreases very slowly or keeps fluctuating even after adjusting the

Iterations = 100
Learning Rate = 0.10
Hidden Nodes = 2
Output Nodes = 1


Data Set

Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions,
precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors.
The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system,
Washington D.C., USA which is publicly available in https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset.
We aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information.


Associated tasks


- Regression: 
Predication of bike rental count hourly or daily based on the environmental and seasonal settings.
	
- Event and Anomaly Detection:  
Count of rented bikes are also correlated to some events in the town which easily are traceable via search engines.
For instance, query like "2012-10-30 washington d.c." in Google returns related results to Hurricane Sandy.
Some of the important events are identified in [1]. 
Therefore the data can be used for validation of anomaly or event detection algorithms as well.



Files
- Readme.md
- hour.csv : bike sharing counts aggregated on hourly basis. Records(rows): 17379 hours, Features (columns): 17.
- day.csv - bike sharing counts aggregated on daily basis. Records (rows): 731 days, Features (columns): 16.

